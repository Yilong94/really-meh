{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corpus = [\"This is a test sentence\",\n",
    "          \"Oranges are my favorite fruit\",\n",
    "          \"I'd like an apple\", \n",
    "          \"An apple a day keeps the doctor away\",\n",
    "          \"Obama speaks to the media in Illinois\",\n",
    "          \"The president greets the press in Chicago\",\n",
    "          \"50 new COVID-19 cases were reported in Singapore today\",\n",
    "         '3 theft cases were reported in Jurong West last week']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TF-IDF method"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!pip3 install sklearn"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: sklearn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.21.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "\u001B[33mWARNING: You are using pip version 21.1.3; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001B[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_doc = 'COVID is a hoax. Blame the chinese'\n",
    "corpus.append(input_doc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vect = TfidfVectorizer(min_df=1, stop_words = 'english')\n",
    "# vect = TfidfVectorizer(min_df=1)\n",
    "tfidf = vect.fit_transform(corpus)\n",
    "pairwise_similarity = tfidf * tfidf.T\n",
    "\n",
    "arr = pairwise_similarity.toarray()\n",
    "np.fill_diagonal(arr, np.nan)\n",
    "\n",
    "input_idx = corpus.index(input_doc)\n",
    "result_idx = np.nanargmax(arr[input_idx])\n",
    "corpus[result_idx]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "arr"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corpus.pop()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence-Transformers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_doc = 'The detective solved two murder cases within the past week'\n",
    "corpus.append(input_doc)\n",
    "\n",
    "#Encoding:\n",
    "embeddings = model.encode(corpus)\n",
    "embeddings.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = cosine_similarity([embeddings[-1]],embeddings[:-1])\n",
    "corpus.pop() # remove new entry from corpus list\n",
    "result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result1 = corpus[np.argmax(result)]\n",
    "result2 = corpus[np.argsort(-result)[0][:3][1]]\n",
    "result3 = corpus[np.argsort(-result)[0][:3][2]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Soft Cosine Measure"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim import corpora\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk\n",
    "# Import and download stopwords from NLTK.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "nltk.download('stopwords')  # Download stopwords list.\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(sentence):\n",
    "    return [w for w in sentence.lower().split() if w not in stop_words]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "documents = []\n",
    "for i in range(len(corpus)):\n",
    "    documents.append(preprocess(corpus[i]))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(documents)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(documents)\n",
    "\n",
    "bow = []\n",
    "for doc in documents:\n",
    "    doc = dictionary.doc2bow(doc)\n",
    "    bow.append(doc)\n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "tfidf = TfidfModel(bow)\n",
    "\n",
    "out = []\n",
    "for b in bow:\n",
    "    b = tfidf[b]\n",
    "    out.append(b)\n",
    "out[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('word2vec-google-news-300')\n",
    "\n",
    "from gensim.similarities import SparseTermSimilarityMatrix, WordEmbeddingSimilarityIndex\n",
    "termsim_index = WordEmbeddingSimilarityIndex(model)\n",
    "termsim_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary, tfidf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "similarity = termsim_matrix.inner_product(out[-1], out[6], normalized=(True, True))\n",
    "print('similarity = %.4f' % similarity)\n",
    "termsim_matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim.similarities import SoftCosineSimilarity\n",
    "#Calculate Soft Cosine Similarity between the query and the documents.\n",
    "def find_similarity(query,documents):\n",
    "    query = preprocess(query)\n",
    "    query = dictionary.doc2bow(query)\n",
    "    index = SoftCosineSimilarity(\n",
    "        [dictionary.doc2bow(document) for document in documents],\n",
    "        termsim_matrix)\n",
    "    return index[query]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc = 'COVID is a hoax. Blame the chinese'\n",
    "\n",
    "find_similarity(doc, documents)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corpus"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}