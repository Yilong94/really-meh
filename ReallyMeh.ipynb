{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "corpus = [\"Jake Peralta is the best detective in New York.\",\n",
    "          \"Oranges are my favorite fruit\",\n",
    "          \"I'd like an apple\", \n",
    "          \"An apple a day keeps the doctor away\",\n",
    "          \"Obama speaks to the media in Illinois\",\n",
    "          \"The president greets the press in Chicago\",\n",
    "          \"50 new COVID-19 cases were reported in Singapore today\",\n",
    "         '3 theft cases were reported in Jurong West last week']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TF-IDF method"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip3 install sklearn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "def find_sim(input_doc, corpus):\n",
    "    corpus.append(input_doc)\n",
    "    vect = TfidfVectorizer(min_df=1, stop_words = 'english')\n",
    "    tfidf = vect.fit_transform(corpus)\n",
    "    \n",
    "    sim = cosine_similarity(tfidf[-1], tfidf[:-1])\n",
    "    print(sim)\n",
    "    \n",
    "    top3 = np.argsort(-sim)[0][:3].tolist()\n",
    "    for i in top3:\n",
    "        print(corpus[i])\n",
    "    corpus.pop()\n",
    "    return top3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "input_doc = 'The detective solved two murder cases within the past week'\n",
    "find_sim(input_doc, corpus)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.1374178  0.         0.         0.         0.         0.\n",
      "  0.0916917  0.25224758]]\n",
      "3 theft cases were reported in Jurong West last week\n",
      "Jake Peralta is the best detective in New York.\n",
      "50 new COVID-19 cases were reported in Singapore today\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[7, 0, 6]"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentence-Transformers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "input_doc = 'The detective solved two murder cases within the past week'\n",
    "corpus.append(input_doc)\n",
    "\n",
    "#Encoding:\n",
    "embeddings = model.encode(corpus)\n",
    "embeddings.shape"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/gj/85c45c1d1zg7r1w0lxs4lcp40000gn/T/ipykernel_3918/1441415466.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;31m#Encoding:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0membeddings\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0membeddings\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = cosine_similarity([embeddings[-1]],embeddings[:-1])\n",
    "corpus.pop() # remove new entry from corpus list\n",
    "result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result1 = corpus[np.argmax(result)]\n",
    "result2 = corpus[np.argsort(-result)[0][:3][1]]\n",
    "result3 = corpus[np.argsort(-result)[0][:3][2]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Soft Cosine Measure"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim import corpora\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk\n",
    "# Import and download stopwords from NLTK.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "nltk.download('stopwords')  # Download stopwords list.\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(sentence):\n",
    "    return [w for w in sentence.lower().split() if w not in stop_words]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "documents = []\n",
    "for i in range(len(corpus)):\n",
    "    documents.append(preprocess(corpus[i]))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(documents)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(documents)\n",
    "\n",
    "bow = []\n",
    "for doc in documents:\n",
    "    doc = dictionary.doc2bow(doc)\n",
    "    bow.append(doc)\n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "tfidf = TfidfModel(bow)\n",
    "\n",
    "out = []\n",
    "for b in bow:\n",
    "    b = tfidf[b]\n",
    "    out.append(b)\n",
    "out[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('word2vec-google-news-300')\n",
    "\n",
    "from gensim.similarities import SparseTermSimilarityMatrix, WordEmbeddingSimilarityIndex\n",
    "termsim_index = WordEmbeddingSimilarityIndex(model)\n",
    "termsim_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary, tfidf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "similarity = termsim_matrix.inner_product(out[-1], out[6], normalized=(True, True))\n",
    "print('similarity = %.4f' % similarity)\n",
    "termsim_matrix"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gensim.similarities import SoftCosineSimilarity\n",
    "#Calculate Soft Cosine Similarity between the query and the documents.\n",
    "def find_similarity(query,documents):\n",
    "    query = preprocess(query)\n",
    "    query = dictionary.doc2bow(query)\n",
    "    index = SoftCosineSimilarity(\n",
    "        [dictionary.doc2bow(document) for document in documents],\n",
    "        termsim_matrix)\n",
    "    return index[query]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc = 'COVID is a hoax. Blame the chinese'\n",
    "\n",
    "find_similarity(doc, documents)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corpus"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}