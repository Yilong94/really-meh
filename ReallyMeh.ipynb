{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fcec5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"This is a test sentence\",\n",
    "          \"Oranges are my favorite fruit\",\n",
    "          \"I'd like an apple\", \n",
    "          \"An apple a day keeps the doctor away\",\n",
    "          \"Obama speaks to the media in Illinois\",\n",
    "          \"The president greets the press in Chicago\",\n",
    "          \"50 new COVID-19 cases were reported in Singapore today\",\n",
    "         '3 theft cases were reported in Jurong West last week']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c35d54",
   "metadata": {},
   "source": [
    "## TF-IDF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad0eb28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4036144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_doc = 'COVID is a hoax. Blame the chinese'\n",
    "corpus.append(input_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5202ebe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50 new COVID-19 cases were reported in Singapore today'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(min_df=1, stop_words = 'english')\n",
    "# vect = TfidfVectorizer(min_df=1)\n",
    "tfidf = vect.fit_transform(corpus)\n",
    "pairwise_similarity = tfidf * tfidf.T\n",
    "\n",
    "arr = pairwise_similarity.toarray()\n",
    "np.fill_diagonal(arr, np.nan)\n",
    "\n",
    "input_idx = corpus.index(input_doc)\n",
    "result_idx = np.nanargmax(arr[input_idx])\n",
    "corpus[result_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c559de46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       nan, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        ,        nan, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        ,        nan, 0.25103029, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.25103029,        nan, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ,        nan,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "               nan, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ,        nan, 0.22920576, 0.13854187],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.22920576,        nan, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.13854187, 0.        ,        nan]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e64d9b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COVID is a hoax. Blame the chinese'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003f4fee",
   "metadata": {},
   "source": [
    "## Sentence-Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99255a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a24dd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9949cf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_doc = 'The detective solved two murder cases within the past week'\n",
    "corpus.append(input_doc)\n",
    "\n",
    "#Encoding:\n",
    "embeddings = model.encode(corpus)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2786c1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01250362,  0.12978113, -0.00957555,  0.29170883,  0.29865646,\n",
       "         0.29103833,  0.4675786 ,  0.71766376]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = cosine_similarity([embeddings[-1]],embeddings[:-1])\n",
    "corpus.pop() # remove new entry from corpus list\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54245afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = corpus[np.argmax(result)]\n",
    "result2 = corpus[np.argsort(-result)[0][:3][1]]\n",
    "result3 = corpus[np.argsort(-result)[0][:3][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fe04bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3 theft cases were reported in Jurong West last week'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb7cd9",
   "metadata": {},
   "source": [
    "## Soft Cosine Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac263b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import gensim.downloader as api\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "717756a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mingshuseah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Import and download stopwords from NLTK.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "nltk.download('stopwords')  # Download stopwords list.\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(sentence):\n",
    "    return [w for w in sentence.lower().split() if w not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e395e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i in range(len(corpus)):\n",
    "    documents.append(preprocess(corpus[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a2fdbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['test', 'sentence'], ['oranges', 'favorite', 'fruit'], [\"i'd\", 'like', 'apple'], ['apple', 'day', 'keeps', 'doctor', 'away'], ['obama', 'speaks', 'media', 'illinois'], ['president', 'greets', 'press', 'chicago'], ['50', 'new', 'covid-19', 'cases', 'reported', 'singapore', 'today'], ['3', 'theft', 'cases', 'reported', 'jurong', 'west', 'last', 'week']]\n"
     ]
    }
   ],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02949596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0.7071067811865476), (1, 0.7071067811865476)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(documents)\n",
    "\n",
    "bow = []\n",
    "for doc in documents:\n",
    "    doc = dictionary.doc2bow(doc)\n",
    "    bow.append(doc)\n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "tfidf = TfidfModel(bow)\n",
    "\n",
    "out = []\n",
    "for b in bow:\n",
    "    b = tfidf[b]\n",
    "    out.append(b)\n",
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a511031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 33/33 [00:15<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('word2vec-google-news-300')\n",
    "\n",
    "from gensim.similarities import SparseTermSimilarityMatrix, WordEmbeddingSimilarityIndex\n",
    "termsim_index = WordEmbeddingSimilarityIndex(model)\n",
    "termsim_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88c9578b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity = 0.1667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.similarities.termsim.SparseTermSimilarityMatrix at 0x12eaf4ee0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = termsim_matrix.inner_product(out[-1], out[6], normalized=(True, True))\n",
    "print('similarity = %.4f' % similarity)\n",
    "termsim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b8547a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.similarities import SoftCosineSimilarity\n",
    "#Calculate Soft Cosine Similarity between the query and the documents.\n",
    "def find_similarity(query,documents):\n",
    "    query = preprocess(query)\n",
    "    query = dictionary.doc2bow(query)\n",
    "    index = SoftCosineSimilarity(\n",
    "        [dictionary.doc2bow(document) for document in documents],\n",
    "        termsim_matrix)\n",
    "    return index[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7299177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0., dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = 'COVID is a hoax. Blame the chinese'\n",
    "\n",
    "find_similarity(doc, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f54a002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a test sentence',\n",
       " 'Oranges are my favorite fruit',\n",
       " \"I'd like an apple\",\n",
       " 'An apple a day keeps the doctor away',\n",
       " 'Obama speaks to the media in Illinois',\n",
       " 'The president greets the press in Chicago',\n",
       " '50 new COVID-19 cases were reported in Singapore today',\n",
       " '3 theft cases were reported in Jurong West last week']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
